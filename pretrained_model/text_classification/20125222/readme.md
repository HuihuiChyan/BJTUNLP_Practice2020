## 本程序为实现使用BERT进行文本分类
### 数据集合处理：利用bert的分词器进行处理


## 模型描述：
### 使用的bert-base-uncased
### 实现：仅添加一层全联接
### 参数：本次结果未调参数，1e-5左右学习率，3轮内即可收敛至92%以上，调参数可能会更高
### 批次：4，近期服务器用的比较多，好不容易才抢到1/3张卡，所以批次开的有点小
### 使用批次数进行验证集测试，因为基本几轮就收敛了，批次数%100或者200进行测试更好点吧...不太会解释这部分...


## 后期改进：
#### 调参数以提高test_accc：学习率、批次、dropout等
#### 尝试复杂模型：BERT+CNN/RNN等结构
#### 尝试推荐的简单方法：以输出的第一位作为分类结果


## 补充：
BERT牛逼！！！直接输入句子，中间不需要处理即可变成需要的索引值的类型、模型几轮内就可以收敛的很好、效果比传统模型大多数情况下都好

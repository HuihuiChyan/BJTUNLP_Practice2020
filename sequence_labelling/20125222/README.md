## 0 介绍
CRF使用的是开源库进行，其代码内部细节不知，对于模型优化感觉束手无策，仅通过调参数进行优化。

## 1 数据处理

#### 第一版代码：标点分割、转化为小写、缩略词处理（非缩略词词展开，而是存在类似于's->is,'m->am）、填充切割。根据数据统计，设置句子长度为600时，仅不足十个训练样本需要截断（长度为300时也满足，具体数据忘了...）
#### 不足和展望：增加词性还原（已实现，但是时间成本有点高，后续改进代码后再进行试验测试）

## 2 模型内容
#### 第一版代码：BiLSTM+CRF结果（复现论文，但是效果远远达不到论文效果）
#### 不足和展望：直接调库实现，内部细节不知，目前在论文基础上进行优化尚无思路，可能对其增加层数/增加模型的复杂性来进行尝试优化

## 3 参数设置
#### 第一版代码：使用Adam,学习率设置为0.0005-0.00001之间寻找，dropout=0.5，batchsize=256，使用了权重衰减
#### 运行过程中...显卡无人使用时，循环n次初始化模型去寻找最优结果

##### 注意：由于输入和输出为定长（大量补充的标签），所以需要设置一个mask，评估时需要只评估[:mask]的部分，后面大量补充的无意义值，会导致评分贼高，但是实际测试很低（随便运行模型，F1:87%，真实测试的时候仅有60%）；结果出来后需要特殊处理：例如还原成原先长度、替换UNK、缺乏值用标签中数量最多的标签进行替换等.听说有变长的输入操作，后续进行研究。
## 4 待解决问题
#### 实际测试比自己跑时评分低很多（80%->76%）